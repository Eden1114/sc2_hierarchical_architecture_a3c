已完成部分：
1）上下层网络具体什么时候更新有待商榷
——目前写成了
    上层网络在一个宏动作执行失败or成功执行后立刻更新（输入参数为宏动作执行时间内所有帧(last_timesteps[0], action[0], timesteps[0])的信息）
    下层网络在其生成的坐标被【采用】后立刻更新（输入参数为坐标被采用的【唯一】一帧里(last_timesteps[0], action[0], timesteps[0])的信息）


待修改的小部分：
1）main的runThread中的4个learningRate和Discount，也可以修改


待修改的重点部分：
1）动作硬编码的设计（宏动作）——已经写好了6个基本的

2）上下层网络结构（即具体怎么卷积、怎么全连接）有待修改——done
3）2个网络结构接收的参数（信息）也应该有所不同（改info）
3）step_low的接收参数里应该还有dir_hig和当前执行的action_id 待改——done（但其实可以考虑把action_id替换为ind_todo）

4）Update（low和high）跑通了，但99%没有效果，要改2个模块中的reward——done
5）Reward和value的归一化——done
6）宏动作失败后给惩罚，需要纳入reward中——done
7）low_reward 的设计原则应当是“该reward能帮agent决定出更好的位置”，若low_reward没有这种作用则应考虑重新设计——done

8）编写输出日志的内容（为了数据可视化&性能分析）【可以最后再加】


有可能需要解决的问题：
1）画框这个action而言，下层网络只返回一个坐标，等于画框跟选点没有任何区别了（因为两个点重合），有待解决这个问题
2）a3c_agent.py的build_model中self.dir_high_usedToFeedLowNet和self.act_id的维度固定为了（1,1），而不是（none，1），
不然会报错（原因不明）。所以说，在目前 更新只使用一帧数据的这个机制下，没什么问题，但如果以后update_low要使用多帧数据的话，则还需要改build_model和
    step_low的feed部分和update_low中feed部分


